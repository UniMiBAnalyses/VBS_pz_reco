{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a risolvere lo stesso problema precedente attraverso algoritmi di grid search per la scelta della rete neurale migliore. E' una ricerca molto più dispendiosa a livello di uso di memoria in quanto vengono testate e validate diverse reti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importazione delle librerie usate con le relative versioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.15.4\n",
      "scipy: 1.1.0\n",
      "matplotlib: 3.0.1\n",
      "iPython: 7.2.0\n",
      "scikit-learn: 0.20.1\n",
      "tensorflow: 1.12.0\n",
      "keras: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print('numpy:', numpy.__version__)\n",
    "\n",
    "import scipy\n",
    "print('scipy:', scipy.__version__)\n",
    "\n",
    "import matplotlib\n",
    "print('matplotlib:', matplotlib.__version__)\n",
    "\n",
    "import IPython\n",
    "print('iPython:', IPython.__version__)\n",
    "\n",
    "import sklearn\n",
    "print('scikit-learn:', sklearn.__version__)\n",
    "\n",
    "import tensorflow\n",
    "print('tensorflow:', tensorflow.__version__)\n",
    "\n",
    "import keras\n",
    "print('keras:', keras.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.interpolate import interp1d\n",
    "import scipy.odr as odr \n",
    "from keras.activations import relu\n",
    "from keras.activations import tanh\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mp\n",
    "import itertools\n",
    "import random\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from array import array\n",
    "from sklearn import metrics\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import svm, datasets\n",
    "from scipy import interp\n",
    "from keras.callbacks import History \n",
    "history = History()\n",
    "from keras.callbacks import *\n",
    "from sklearn.externals import joblib\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import scipy.stats\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo i dati su cui performare la regressione e dividiamo il dataset in training set validation set e test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewk_input = np.load('ewk_signal_withreco_withdelta_input.npy')\n",
    "ewk_truth = np.load('ewk_signal_withreco_withdelta_truth.npy')\n",
    "ewk_input_train = ewk_input[0:400000,:]\n",
    "ewk_truth_train = ewk_truth[0:400000,4].reshape(-1, 1)\n",
    "\n",
    "ewk_input_validation = ewk_input[400000:450000,:]\n",
    "ewk_truth_validation = ewk_truth[400000:450000,4].reshape(-1, 1)\n",
    "\n",
    "ewk_input_test = ewk_input[450000:501328,:]\n",
    "ewk_truth_test = ewk_truth[450000:501328,4].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-scaliamo il dataset in modo da ottenere dati più maneggiabili poichè sono più vicini allo 0.\n",
    "Z-scalare vuol dire fare la seguente cosa:\n",
    "$ data_{new} = \\frac{data_{old}-\\mu}{\\sigma} $\n",
    "Salviamo successivamente il modello scalato in un file 'scaler_input.pkl' in modo da poterlo richiamare successivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_truth.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_input = StandardScaler()\n",
    "scaler_truth = StandardScaler()\n",
    "\n",
    "\n",
    "scaler_input.fit(ewk_input_train)\n",
    "scaler_truth.fit(ewk_truth_train)\n",
    "\n",
    "input_train = scaler_input.transform(ewk_input_train)\n",
    "truth_train = scaler_truth.transform(ewk_truth_train)\n",
    "input_validation = scaler_input.transform(ewk_input_validation)\n",
    "truth_validation = scaler_truth.transform(ewk_truth_validation)\n",
    "input_test = scaler_input.transform(ewk_input_test)\n",
    "truth_test = ewk_truth_test\n",
    "\n",
    "joblib.dump(scaler_input,  \"scaler_input.pkl\")\n",
    "joblib.dump(scaler_truth,  \"scaler_truth.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo adesso la cartella in cui salvare i file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/christian/Scrivania/notebook/VBS analysis/risultati_predizioni/grid_search\"\n",
    "comparison_dir = \"/home/christian/Scrivania/notebook/VBS analysis/risultati_predizioni\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobbiamo creare ora una funzione che contenga il nostro modello di rete neurale cosi da poterlo richiamare nel metodo Keras.Regressor e poter provare i diversi parametri di fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
